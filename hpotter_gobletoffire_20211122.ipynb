{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hpotter_gobletoffire_20211122.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uSuoEy_44px2"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/sentiment_cruxes/blob/main/hpotter_gobletoffire_20211122.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSec38dY4n0y"
      },
      "source": [
        "Jon Chun\n",
        "24 Oct 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vcCvDsM_gKY"
      },
      "source": [
        "# **Setup and Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P16RfkGGJQIW"
      },
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdi73sMwJQDg"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HvDlVcyBomt"
      },
      "source": [
        "!pip install texthero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6KeRBJe_S_g"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojnn0LhdBpqy"
      },
      "source": [
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAVX1ZT7Ccii"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee4FgrcZ57cm"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXp2pqcmyEqc"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5_GRtpVsdgC"
      },
      "source": [
        "# **Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvGgV_xzaece"
      },
      "source": [
        "def twoway_probability2sentiment(text_str, sentiment_2polarity_fn, pol_labels=['negative','positive']):\n",
        "  '''\n",
        "  Given a text string, sentiment_fn (return 0.0 to 1.0) and a list of 2 labels for negative and positive classes\n",
        "    e.g. CamemBERT returns (LABEL_0/LABEL_1) for (Negative/Positive)\n",
        "         xxx returns (LABEL_1/LABEL_2) for (Negative/Positive)\n",
        "         xxx returns (NEGATIVE/POSITIVE)\n",
        "         xxx returns (Neg/Pos)\n",
        "  Get return a sign adjusted sentiment score -1.0 to 1.0\n",
        "  '''\n",
        "\n",
        "  model_score = sentiment_2polarity_fn(text_str)\n",
        "  pol_str = model_score[0]['label']\n",
        "\n",
        "  # print(f'pol_str = {pol_str} and is type:{type(pol_str)}')\n",
        "  score_fl = float(model_score[0]['score'])\n",
        "  # print(f'score_fl = {score_fl} and is type{type(score_fl)}')\n",
        "\n",
        "  # print(f'pol_str.lower: {pol_str.lower()} and pol_labels[0]: {pol_labels[0]}')\n",
        "  if (pol_str.lower() in pol_labels[0].lower()):\n",
        "    # print('negative')\n",
        "    sign_fl = -1.0\n",
        "  elif (pol_str.lower() in pol_labels[1].lower()):\n",
        "    # print('positive')\n",
        "    sign_fl = 1.0\n",
        "  else:\n",
        "    print(f'ERROR polarity string: {pol_str} must be one of two values (e.g. [Nn]egative|[Pp]ositive)')\n",
        "    return -99\n",
        "    \n",
        "  return sign_fl * score_fl\n",
        "\n",
        "# Test\n",
        "# test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n",
        "# print(f'test_fl: {test_fl}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi-N1Z9Q1DFf"
      },
      "source": [
        "def threeway_probability2sentiment(text_str, sentiment_2polarity_fn):\n",
        "  '''\n",
        "  Given a text string and sentiment_fn that returns ['negative|positive|neutral', float(0.0-1.0)]\n",
        "  Get return a sign adjusted sentiment score -1.0 to 1.0\n",
        "  '''\n",
        "  sign_fl = 1.0\n",
        "  \n",
        "  # Special case for Pysentimiento\n",
        "  score_pysentimiento_fl = -99.0    # Use Pysentimeinto score as flag with val -99.0\n",
        "  if False: # sentiment_2polarity_fn == analyzer.predict:\n",
        "    # from pysentimiento import SentimentAnalyzer\n",
        "    # analyzer = SentimentAnalyzer(lang=\"en\")\n",
        "    # print('Using Pysentimiento')\n",
        "    text_str_ls = text_str.split()[:125]\n",
        "    text_125_str = ' '.join(text_str_ls)\n",
        "    pol_object = analyzer.predict(text_125_str)\n",
        "    pol_str = pol_object.output\n",
        "    if pol_str == 'NEG':\n",
        "      sign_fl = -1.0\n",
        "    elif pol_str == 'NEU':\n",
        "      sign_fl = 1.0\n",
        "    else:\n",
        "      # Polarity is 'POS' by default\n",
        "      sign_fl = 1.0\n",
        "    score_pysentimiento_fl = sign_fl * pol_object.probas[pol_str]\n",
        "\n",
        "    # Distribute the Neutral values between -0.5 and +0.5\n",
        "    if pol_str == 'NEU':\n",
        "      score_pysentimiento_fl = score_pysentimiento_fl - 0.5\n",
        "\n",
        "  # General case for other 3-way sentiment models\n",
        "  else:\n",
        "    # print('Not using Pysentimiento')\n",
        "    model_score = sentiment_2polarity_fn(text_str)\n",
        "    pol_str = model_score[0]['label']\n",
        "    # print(f'pol_str = {pol_str} and is type:{type(pol_str)}')\n",
        "    score_fl = float(model_score[0]['score'])\n",
        "    # print(f'score_fl = {score_fl} and is type{type(score_fl)}')\n",
        "\n",
        "    if (pol_str.lower().startswith('neu')) | (pol_str in ['NEU','LABEL_0']):\n",
        "      # print('negative')\n",
        "      if score_fl < 0.5:\n",
        "        sign_fl = -1.0\n",
        "      else:\n",
        "        sign_fl = +1.0\n",
        "      adj_base = 0.0\n",
        "    elif (pol_str.lower().startswith('neg')) | (pol_str in ['NEG','LABEL_1']):\n",
        "      # print('positive')\n",
        "      sign_fl = -1.0\n",
        "      adj_base = -1.0\n",
        "    elif (pol_str.lower().startswith('pos')) | (pol_str in ['POS','LABEL_2']):\n",
        "      # print('positive')\n",
        "      sign_fl = 1.0\n",
        "      adj_base = 1.0\n",
        "    else:\n",
        "      print(f'ERROR polarity string: {pol_str} must be one of two values (e.g. [Nn]egative|[Pp]ositive)')\n",
        "\n",
        "  if score_pysentimiento_fl == -99.0:\n",
        "    adj_score = (sign_fl * score_fl) + adj_base\n",
        "  else:\n",
        "    adj_score = score_pysentimiento_fl\n",
        "\n",
        "  return adj_score # , adj_base\n",
        "\n",
        "# Test\n",
        "# test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n",
        "# print(f'test_fl: {test_fl}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SguJaRegHZY"
      },
      "source": [
        "def fiveway_probability2sentiment(text_str, sentiment_5star_fn):\n",
        "  '''\n",
        "  Given a text string and sentiment_fn that returns '1 star' to '5 stars' rating with probability]\n",
        "  Get return a sign adjusted sentiment score 0.0 to 5.0\n",
        "  '''\n",
        "\n",
        "  model_score = sentiment_5star_fn(text_str)\n",
        "  pol_str = model_score[0]['label']\n",
        "  # print(f'pol_str = [{pol_str}]')\n",
        "  if pol_str in ['1 star','LABEL_0']:\n",
        "    score_base = 0.0\n",
        "  elif pol_str in ['2 stars','LABEL_1']:\n",
        "    score_base = 1.0\n",
        "  elif pol_str in ['3 stars','LABEL_2']:\n",
        "    score_base = 2.0\n",
        "  elif pol_str in ['4 stars','LABEL_3']:\n",
        "    score_base = 3.0\n",
        "  elif pol_str in ['5 stars','LABEL_4']:\n",
        "    score_base = 4.0\n",
        "  else:\n",
        "    print(f\"ERROR: polarity string = {pol_str} must be in [1-5] 'stars'\")\n",
        "    score_base = 2.0\n",
        "\n",
        "  score_fl = score_base + model_score[0]['score']\n",
        "\n",
        "  return score_fl\n",
        "\n",
        "# Test\n",
        "# test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n",
        "# print(f'test_fl: {test_fl}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etz3zVn3kTe-"
      },
      "source": [
        "# **Get the Novel Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9E7X7zYDJP0"
      },
      "source": [
        "## **OPTION (a): Connect and Read from gDrive** *italicized text*\n",
        "\n",
        "(Upload novel textfile to Google Drive under root folder **./MyDrive** first)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IUgIHtDK6p"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzjtyT8cDXjh"
      },
      "source": [
        "# drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTfKyeitDdss"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jK4jHomDlWH"
      },
      "source": [
        "%cd ./MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duLpLKgiDutf"
      },
      "source": [
        "# %cd ./research/2021/sa_book_code/books_sa/bsmith_atreegrowsinbrooklyn/\n",
        "\n",
        "%cd ./research/2021/sa_book_code/books_sa/jkrowling_4gobletoffire/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYDzPNVrEAb9"
      },
      "source": [
        "!ls *.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjm-P2FPEAR1"
      },
      "source": [
        "novel_ls = []\n",
        "novel_parags_ls = []\n",
        "\n",
        "novel_filename = 'screenplay_potter_goblet_of_fire.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqpe1TBUDRsp",
        "cellView": "form"
      },
      "source": [
        "#@title Enter the Novel_Title in the form [Title] by [Author]\n",
        "\n",
        "\n",
        "Novel_Title = \"Harry Potter and the Goblet of Fire by J.K. Rowling\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3_1O-epYu7-"
      },
      "source": [
        "# Read novel into list of Sentences/lines\n",
        "\n",
        "with open(novel_filename, 'r') as fp:\n",
        "  novel_ls = fp.readlines()\n",
        "\n",
        "print(f'Line Count: {len(novel_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESu4h7F6prob"
      },
      "source": [
        "print(novel_ls[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEx5GepxpQ-I"
      },
      "source": [
        "def strip_ascii(text):\n",
        "  return \"\".join(\n",
        "    char for char\n",
        "    in text\n",
        "    if 31 < ord(char) < 127\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eFRP9_YpXGs"
      },
      "source": [
        "# Strip out non-printable ASCII\n",
        "\n",
        "# novel_ls = [x.encode('ascii',errors='ignore').decode() for x in novel_ls]\n",
        "\n",
        "novel_ls = [strip_ascii(x) for x in novel_ls]\n",
        "novel_ls = [x for x in novel_ls if len(x) > 0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc8cLoBIoi_i"
      },
      "source": [
        "print(novel_ls[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaXfSTkcYo5y"
      },
      "source": [
        "# Read novel into list of Paragraphs\n",
        "\n",
        "delimiter = \"\\n\\n\"\n",
        "\n",
        "with open(novel_filename, \"r\") as fp:\n",
        "  all_content = fp.read() #reading all the content in one step\n",
        "  #using the string methods we split it\n",
        "  novel_parags_ls = all_content.split(delimiter)\n",
        "  novel_parags_ls = [x.strip() for x in novel_parags_ls if len(x.strip()) > 2]\n",
        "\n",
        "print(f'Paragraph Count: {len(novel_parags_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFxzh810ZOb3"
      },
      "source": [
        "novel_parags_ls[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39YMpm9-Xyft"
      },
      "source": [
        "novel_ls[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltHD0CVWnqrZ"
      },
      "source": [
        "novel_clean_str = '\\n'.join(novel_ls)\n",
        "\n",
        "print(novel_clean_str[:5000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwW1XnEE_Z5_"
      },
      "source": [
        "## OPTION (b): **Scrape Project Gutenberg**\n",
        "\n",
        "**Goto *https://gutenberg.net.au* and find the *.HTML (not *.TXT) version of your novel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXUp0wWW4nBt"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqdunORR2tUu"
      },
      "source": [
        "#@title Enter the URL of your novel at ***gutenberg.net.au***\n",
        "#@markdown Paste the URL to the ***HTML version*** (not plain text).\n",
        "\n",
        "Novel_Title = \"The Adventures of Huckleberry Finn by Mark Twain\"  #@param {type: \"string\"}\n",
        "\n",
        "Gutenberg_URL = 'https://gutenberg.org/cache/epub/76/pg76-images.html'  #@param {type: \"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atAOfnhh4m_B"
      },
      "source": [
        "# Get raw HTML of novel from Gutenberg.net.au\n",
        "\n",
        "response=requests.get(Gutenberg_URL)  # TODO: Pass the URL to the .get() method of the requests object\n",
        "html = response.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLnTulWcJumI"
      },
      "source": [
        "# View raw HTML that we need to clean up\n",
        "\n",
        "# TODO: What is the difference between these two outputs?\n",
        "\n",
        "# Option A: \n",
        "html\n",
        "\n",
        "# Option B:\n",
        "# print(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSuoEy_44px2"
      },
      "source": [
        "## **Using Beautiful Soup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNgJejlT4m9C"
      },
      "source": [
        "#Create a BeautifulSoup object from the HTML\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "paragraph=soup.find_all(\"p\")  # TODO: get all the <P>Paragraphs</P> \n",
        "                                #       see bs4 API ref: https://beautiful-soup-4.readthedocs.io/en/latest/#kinds-of-objects\n",
        "parag_ls = []\n",
        "for para in paragraph:\n",
        "    parag_ls.append(para.text)\n",
        "\n",
        "print(f'There were {len(parag_ls)} Paragraphs:\\n') # TODO how do you get the number of paragraphs in the list parag_ls?\n",
        "\n",
        "print(f\"First 3 Paragraphs: ==============================    \\n\")\n",
        "print(f\"    {list(print(x) for x in parag_ls[:3])}\\n\")  # TODO: Give index to retrieve the first 3 paragraphs\n",
        "\n",
        "print(f\"Last 3 Paragraphs: ============================== \\n\")\n",
        "print(f\"    {list(print(x) for x in parag_ls[-3:])}\\n\")  # TODO: Give index to retrieve the last 3 paragraphs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmY3yPHD4u2d"
      },
      "source": [
        "## **Using Python [string].partition() or RegEx**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z67_hRa5aai"
      },
      "source": [
        "# Concatenate all paragraphs into a single novel string\n",
        "\n",
        "# For every paragraph, replace all hardcoded \\r\\n with a single space\n",
        "parag_flat_ls = [re.sub(r'\\r\\n', ' ', aparag) for aparag in parag_ls]\n",
        "\n",
        "# Concatenate all paragraphs into a single string, separated by \\n\n",
        "novel_str = '\\n'.join(parag_flat_ls)\n",
        "\n",
        "print('\\nSTART OF NOVEL: -----')\n",
        "print(novel_str[:1000] + '\\n')\n",
        "\n",
        "print('\\nEND OF NOVEL: -----\\n')\n",
        "print(novel_str[-1000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osCCqMaeueLo"
      },
      "source": [
        "**Enter the First and Last several words to use as RegEx for trimming header/footers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDGA7bQC4yKi",
        "cellView": "form"
      },
      "source": [
        "#@title Enter the first sentence in the body of your novel\n",
        "sentence_first_str = 'You don\\u2019t know about me without'  #@param {type: \"string\"}\n",
        "\n",
        "#@title Enter the last sentence in the body of your novel\n",
        "sentence_last_str = 'stand it. I been there before.'  #@param {type: \"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oplsBBKu8ZTu"
      },
      "source": [
        "# Strip off the header\n",
        "novel_clean = ' '.join(novel_str.partition(sentence_first_str)[1:])\n",
        "\n",
        "# Strip off the footer\n",
        "' '.join(novel_clean.partition(sentence_last_str)[:2])[-500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eoj4RfYA7I9r"
      },
      "source": [
        "# Strip off the header\n",
        "novel_clean_str = ' '.join(novel_str.partition(sentence_first_str)[1:])\n",
        "\n",
        "# Strip off the footer\n",
        "novel_clean_str = ' '.join(novel_clean_str.partition(sentence_last_str)[:2])\n",
        "\n",
        "# Verify\n",
        "\n",
        "print('\\nSTART OF CLEAN NOVEL: -----')\n",
        "print(novel_clean_str[:1000] + '\\n')\n",
        "\n",
        "print('\\nEND OF CLEAN NOVEL: -----\\n')\n",
        "print(novel_clean_str[-1000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfJDM5xb9Q4P"
      },
      "source": [
        "# **Split Novel into Sentences**\n",
        "\n",
        "* https://github.com/zaemyung/sentsplit (CRF: mincut)\n",
        "\n",
        "* https://github.com/adobe/NLP-Cube and Rank ~15 https://aclanthology.org/K18-2017.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fzfxb9GvNIS"
      },
      "source": [
        "# Read novel into list of Paragraphs\n",
        "\n",
        "delimiter = \"\\n\"\n",
        "\n",
        "novel_parags_ls = novel_clean_str.split(delimiter)\n",
        "novel_parags_ls = [x.strip() for x in novel_parags_ls if len(x.strip()) > 2]\n",
        "novel_parags_ls = [' '.join(x.split()) for x in novel_parags_ls]\n",
        "\n",
        "print(f'Paragraph Count: {len(novel_parags_ls)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkociaLFAcDk"
      },
      "source": [
        "novel_parags_ls[:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgBtKrBzADqS"
      },
      "source": [
        "for i,aline in enumerate(novel_parags_ls):\n",
        "  if (len(aline.strip()) < 5):\n",
        "    print(f'Line #{i}: {aline}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sK_aIGvqbWM"
      },
      "source": [
        "# Prior several code blocks for future functionality, can start execution in this section with cell below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GIUTDNnY94U"
      },
      "source": [
        "novel_clean_str = '\\n'.join(novel_parags_ls)\n",
        "novel_clean_str[:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvByxI4s9Qb7"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azKvgIBN9QYF"
      },
      "source": [
        "novel_sents_ls = sent_tokenize(novel_clean_str)\n",
        "\n",
        "sent_ct = len(novel_sents_ls)\n",
        "sent_show = 10\n",
        "\n",
        "print('\\nFirst Sentences: -----\\n')\n",
        "# for i, asent in enumerate(novel_sents_ls[:sent_show]):\n",
        "for i, asent in enumerate(novel_sents_ls[:sent_show]):\n",
        "  print(f'Sentences #{i}: {asent}')\n",
        "\n",
        "\n",
        "print('\\nLast Sentences: -----\\n')\n",
        "for i, asent in enumerate(novel_sents_ls[-sent_show:]):\n",
        "  print(f'Sentences #{sent_ct - (sent_show - i)}: {asent}')\n",
        "\n",
        "\n",
        "print(f'\\n\\nThere are {sent_ct} Sentences in the novel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS9K36w2-f6X"
      },
      "source": [
        "# View the Sentences that have no letters in them\n",
        "\n",
        "[x.strip() for x in novel_sents_ls if not re.search('[a-zA-Z]', x)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FLOEuUp9ml-"
      },
      "source": [
        "# Delete the short Sentences and those without any alphabetic characters\n",
        "\n",
        "novel_sents_ls = [x.strip() for x in novel_sents_ls if len(x.strip()) > 2]\n",
        "novel_sents_ls = [x.strip() for x in novel_sents_ls if re.search('[a-zA-Z]', x)]\n",
        "len(novel_sents_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPzyga1l8Qg4"
      },
      "source": [
        "# View the shortest Setences\n",
        "\n",
        "sorted(novel_sents_ls, key=len)[:100]\n",
        "# type(min(novel_sents_ls, key=len))\n",
        "# novel_sents_ls[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Z0fo0t5HqS"
      },
      "source": [
        "len(novel_sents_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFv-UmuUnCeb"
      },
      "source": [
        "# **Expand Contractions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy1PPpdTnyGp"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usyt6b-ySF8w"
      },
      "source": [
        "import contractions\n",
        "contractions.fix(\"you're happy now\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8bQOUGEwE4"
      },
      "source": [
        "# novel_clean_ls = [re.sub(r'[\\n]+', ' ', x).strip() for x in novel_ls]\n",
        "novel_clean_ls = [contractions.fix(x) for x in novel_ls]\n",
        "novel_clean_ls = [re.sub(r'[\\n]+', ' ', x).strip() for x in novel_clean_ls]\n",
        "novel_clean_ls = [x.strip() for x in novel_clean_ls if len(x.strip()) > 1]\n",
        "# novel_clean_ls = [re.sub(r\"^[\\\"\\']\", \"\", x) for x in novel_clean_ls]  # re.sub(\"[\\\"\\']\", \"\", s)\n",
        "# novel_clean_ls = [re.sub(r\"[\\\"\\']$\", \"\", x) for x in novel_clean_ls]\n",
        "novel_clean_ls = [x.encode('ascii',errors='ignore').decode() for x in novel_clean_ls]\n",
        "# novel_bin = novel_clean_str.encode('ascii',errors='ignore')\n",
        "# novel_clean_str = novel_bin.decode()\n",
        "\n",
        "[f'[{x}]' for x in novel_clean_ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofRWGlrZEwSp"
      },
      "source": [
        "novel_clean_str = '\\n'.join(novel_clean_ls)\n",
        "print(novel_clean_str[:5000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcMY8iayIt3a"
      },
      "source": [
        "# novel_clean_ls[1] = \"THE SILVER SPOON I was born twice: first, as a baby girl, on a remarkably smogless Detroit day in January of 1960; and then again, as a teenage boy, in an emergency room near Petoskey, Michigan, in August of 1974. Specialized readers may have come across me in Dr. Peter Luce’s study, “Gender Identity in 5-Alpha-Reductase Pseudohermaphrodites,” published in theJournal of Pediatric Endocrinology in 1975. Or maybe you’ve seen my photograph in chapter sixteen of the now sadly outdatedGenetics and Heredity.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5s060JFJRel"
      },
      "source": [
        "# novel_clean_ls.pop(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyQ-8hAqGTQh"
      },
      "source": [
        "novel_clean_ls[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PLg65IAJV9B"
      },
      "source": [
        "novel_clean_ls[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43C38istJd_2"
      },
      "source": [
        "# novel_clean_str = '\\n'.join(novel_clean_ls)\n",
        "\n",
        "# novel_bin = novel_clean_str.encode('ascii',errors='ignore')\n",
        "# novel_clean_str = novel_bin.decode()\n",
        "\n",
        "# print(novel_clean_str[:5000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DEmb5ofA2nZ"
      },
      "source": [
        "# **Clean and Slice Strings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3IY766c_o0a"
      },
      "source": [
        "import texthero as hero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dU_gSsi_6gZ"
      },
      "source": [
        "hero.get_default_pipeline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0MRaxjaAfie"
      },
      "source": [
        "# Texthero works on Pandas Series\n",
        "\n",
        "# novel_df = pd.DataFrame({'text_raw': novel_sents_ls})\n",
        "novel_df = pd.DataFrame({'text_raw': novel_clean_ls})\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A83XGpXLCGXO"
      },
      "source": [
        "# Convert string column/Series from 'object' to 'string'\n",
        "\n",
        "novel_df['text_raw'] = novel_df['text_raw'].astype('string')\n",
        "novel_df['text_raw'] = novel_df['text_raw'].str.strip()\n",
        "\n",
        "novel_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtOfnqw-Ckh-"
      },
      "source": [
        "# Use texthero.clean() to clean the 'text_raw' column and create the 'text_clean' column\n",
        "\n",
        "novel_df['text_clean'] = hero.clean(novel_df['text_raw'])\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNciD_AXAova"
      },
      "source": [
        "novel_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMvr_rIpBD1J"
      },
      "source": [
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4NvghD5_QtD"
      },
      "source": [
        "# Delete the (near)null Sentences\n",
        "\n",
        "novel_df['text_raw_len'] = novel_df['text_raw'].apply(lambda x : len(x.strip()))\n",
        "novel_df.head()\n",
        "novel_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEzFvB4tBhl7"
      },
      "source": [
        "# View the shortests Sentences before and after cleaning\n",
        "\n",
        "novel_df.sort_values(by=['text_raw_len']).head(400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyaPfm1PBLZ7"
      },
      "source": [
        "novel_df['text_raw_len'].value_counts().sort_values(na_position='first')[:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ejOLzWAEJT"
      },
      "source": [
        "# Drop Sentence if Raw length < 2\n",
        "\n",
        "novel_df = novel_df[novel_df['text_raw_len'] > 2]\n",
        "novel_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjCE8qyBlVAK"
      },
      "source": [
        "novel_df.text_clean = novel_df.text_clean.astype('string')\n",
        "novel_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46hk6bSDlYge"
      },
      "source": [
        "novel_df.sort_values(by=['text_raw_len']).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBwtuQFiDtxW"
      },
      "source": [
        "# **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mujSX6yGnMzV"
      },
      "source": [
        "## **VADER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT_rpH80BDq2"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBh2N-WkEGWR"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "vader_sa = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__By9FqfEV6R"
      },
      "source": [
        "vader_sa.polarity_scores('I love lint')['compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDDsGnsuEBMO"
      },
      "source": [
        "novel_df['vader'] = novel_df['text_clean'].apply(lambda x : vader_sa.polarity_scores(x)['compound'])\n",
        "novel_df.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_DEOrrfnQKT"
      },
      "source": [
        "## **TextBlob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tMg_wp7nQAW"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8PStLF-nkug"
      },
      "source": [
        "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
        "print(testimonial.sentiment.polarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b5YxuccnYTm"
      },
      "source": [
        "novel_df['textblob'] = novel_df['text_clean'].apply(lambda x : TextBlob(x).sentiment.polarity)\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ova-VoaoLlke"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJbeiU1R_z5y"
      },
      "source": [
        "## **Sentiment Analysis: (5-way) RoBERTa Large 15 Datasets**\n",
        "\n",
        "* https://huggingface.co/siebert/sentiment-roberta-large-english\n",
        "* https://huggingface.co/roberta-base "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyZI9cVw_z5z"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
        "print(sentiment_analysis(\"I love this!\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvxllmCZGnCD"
      },
      "source": [
        "# Direct Test\n",
        "\n",
        "sentiment_analysis('I love wonderful good things')\n",
        "print('\\n')\n",
        "sentiment_analysis('I hate your guts you filthy bastard')\n",
        "print('\\n')\n",
        "sentiment_analysis('It is')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUR-y7lvGnCD"
      },
      "source": [
        "# Test\n",
        "\n",
        "model_adj_score = twoway_probability2sentiment('I love wonderful good things', sentiment_analysis, pol_labels=['NEGATIVE','POSITIVE'])\n",
        "model_adj_score\n",
        "print('\\n')\n",
        "model_adj_score = threeway_probability2sentiment('It is not good', sentiment_analysis) # , pol_labels=['NEGATIVE','POSITIVE'])\n",
        "model_adj_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Z7-N5MGnCD"
      },
      "source": [
        "# Test\n",
        "\n",
        "model_adj_score = twoway_probability2sentiment('I hate your guts you bastard', sentiment_analysis, pol_labels=['NEGATIVE','POSITIVE'])\n",
        "model_adj_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEMcJTUWMGOF"
      },
      "source": [
        "novel_df['roberta15lg'] = novel_df['text_raw'].apply(lambda x : twoway_probability2sentiment(x, sentiment_analysis, pol_labels=['NEGATIVE','POSITIVE']))\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVS0XNVGRGnH"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "win_per = 5\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "\n",
        "novel_df['roberta15lg_gauss05'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDcu67dmUJ02"
      },
      "source": [
        "win_per = 5\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "novel_df['roberta15lg_gauss05'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "\n",
        "win_per = 10\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "novel_df['roberta15lg_gauss10'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "\n",
        "win_per = 15\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "novel_df['roberta15lg_gauss15'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "\n",
        "win_per = 20\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "novel_df['roberta15lg_gauss20'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "\n",
        "win_per = 25\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "novel_df['roberta15lg_gauss25'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "\n",
        "win_per = 30\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "novel_df['roberta15lg_gauss30'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kELiYPWnZMzc"
      },
      "source": [
        "%whos str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_FaMJKwUcfy"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss05', bins=100, color='purple', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='5%')\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss10', bins=100, color='blue', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='10%')\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss15', bins=100, color='violet', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='15%')\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss20', bins=100, color='yellow', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='20%')\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss25', bins=100, color='orange', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='25%')\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss30', bins=100, color='red', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='30%')\n",
        "\n",
        "plt.title(f'{Novel_Title} Sentiment Analysis \\n RoBERTa SMA 5-30% Window')\n",
        "plt.xlabel('Sentiment Polarity')\n",
        "plt.legend(title='SMA Window', loc='best')\n",
        "# sns.histplot(data=novel_df, x='textblob', bins=100, color='blue', alpha=0.3, kde=True)\n",
        "# sns.histplot(data=novel_df, x='vader', bins=100, color='green', alpha=0.3, kde=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exh3o8ESTX0R"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss25', bins=100, color='red', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='RoBERTa 25%')\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss30', bins=100, color='orange', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='RoBERTa 30%')\n",
        "sns.histplot(data=novel_df, x='textblob', bins=100, color='blue', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='TextBlob')\n",
        "sns.histplot(data=novel_df, x='vader', bins=100, color='green', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='VADER')\n",
        "\n",
        "plt.title(f'{Novel_Title} Sentiment Analysis \\n RoBERTa SMA 5-30% Window, TextBlob and VADER')\n",
        "plt.xlabel('Sentiment Polarity')\n",
        "plt.legend(title='SMA Window', loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykBeSxrOZ0Wy"
      },
      "source": [
        "novel_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkyRK_7YalU_"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbhHj_YHZ3zp"
      },
      "source": [
        "col_std_ls = [x for x in novel_df.columns if 'text_' not in x]\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "for acol in col_std_ls:\n",
        "  acol_z = f'{acol}_z'\n",
        "  # scaler.fit(novel_df[acol].values.reshape(-1,1))\n",
        "  novel_df[acol_z] = scaler.fit_transform(novel_df[acol].values.reshape(-1,1))\n",
        "\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHKBuT23biyb"
      },
      "source": [
        "novel_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt_7ww2Sbiul"
      },
      "source": [
        "cols_z_ls = [x for x in novel_df if '_z' in x]\n",
        "cols_z_ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1IimQJxbirb"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss25_z', bins=100, color='red', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='RoBERTa 25%')\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss30_z', bins=100, color='orange', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='RoBERTa 30%')\n",
        "sns.histplot(data=novel_df, x='textblob_z', bins=100, color='blue', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='TextBlob')\n",
        "sns.histplot(data=novel_df, x='vader_z', bins=100, color='green', alpha=0.3, kde=True, line_kws={'linewidth':5}, label='VADER')\n",
        "\n",
        "plt.title(f'{Novel_Title} Sentiment Analysis \\n RoBERTa SMA 5-30% Window, TextBlob and VADER')\n",
        "plt.xlabel('Sentiment Polarity')\n",
        "plt.legend(title='SMA Window', loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvLmp9E2byLu"
      },
      "source": [
        "win_per = 30\n",
        "win_size = int(win_per/100*novel_df.shape[0])\n",
        "mag_factor = 10\n",
        "\n",
        "x_mean = novel_df['textblob_z'].mean()\n",
        "novel_df['textblob_z'].apply(lambda x: mag_factor*(x-x_mean)+x_mean).rolling(win_size, center=True, min_periods=0).mean().plot(label='TextBlob_z', linewidth=5)\n",
        "\n",
        "x_mean = novel_df['vader_z'].mean()\n",
        "novel_df['vader_z'].apply(lambda x: mag_factor*(x-x_mean)+x_mean).rolling(win_size, center=True, min_periods=0).mean().plot(label='VADER_z', linewidth=5)\n",
        "\n",
        "novel_df['roberta15lg_gauss10_z'].plot(label='RoBERTa_10_z', linewidth=5)\n",
        "novel_df['roberta15lg_gauss25_z'].plot(label='RoBERTa_25_z', linewidth=5)\n",
        "novel_df['roberta15lg_gauss30_z'].plot(label='RoBERTa_30_z', linewidth=5)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcZ7t1wUbx_8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SPhxayTbx7y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j0I825xbx16"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs4VbP8LT1DX"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "win_per = 20\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "\n",
        "novel_df['roberta15lg_gauss'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsAEIWQ_T3jw"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "sns.histplot(data=novel_df, x='roberta15lg', bins=100, color='red', alpha=0.3, kde=True)\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss', bins=100, color='orange', alpha=0.3, kde=True)\n",
        "sns.histplot(data=novel_df, x='textblob', bins=100, color='blue', alpha=0.3, kde=True)\n",
        "sns.histplot(data=novel_df, x='vader', bins=100, color='green', alpha=0.3, kde=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxDt7-LqT2U-"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "win_per = 5\n",
        "win_width = int(win_per/100*novel_df.shape[0])\n",
        "\n",
        "novel_df['roberta15lg_gauss'] = novel_df['roberta15lg'].rolling(win_width, center=True, min_periods=0).mean()\n",
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEtum9PtMsnR"
      },
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:\n",
        "\n",
        "sns.histplot(data=novel_df, x='roberta15lg', bins=100, color='red', alpha=0.3, kde=True)\n",
        "sns.histplot(data=novel_df, x='roberta15lg_gauss', bins=100, color='orange', alpha=0.3, kde=True)\n",
        "sns.histplot(data=novel_df, x='textblob', bins=100, color='blue', alpha=0.3, kde=True)\n",
        "sns.histplot(data=novel_df, x='vader', bins=100, color='green', alpha=0.3, kde=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KA5XQ3SMsa8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exzC1W-tuDca"
      },
      "source": [
        "%%time \n",
        "\n",
        "# NOTE:\n",
        "\n",
        "# Read the *.txt file, calculate line sentiment and write out results as *.csv\n",
        "\n",
        "for i, filename_text in enumerate(filenames_clean_ls):\n",
        "\n",
        "  print(f'\\n\\nOPENING FILE #{i}: {filename_text} ==========\\n')\n",
        "  with open(filename_text, 'r+') as fp:\n",
        "    file_lines_ls = fp.readlines()\n",
        "\n",
        "  sentiment_all_ls = []\n",
        "  for j,aline in enumerate(file_lines_ls):\n",
        "    \n",
        "    sentiment_aline = twoway_probability2sentiment(aline, sentiment_analysis, pol_labels=['NEGATIVE','POSITIVE'])\n",
        "    sentiment_all_ls.append(sentiment_aline)\n",
        "    print(f'Line #{j}: Sentiment={sentiment_aline}\\n{aline}\\n')\n",
        "\n",
        "  filename_sentiment = filename_text.replace(\".txt\", \".csv\")\n",
        "  sentiment_dt = {'text': file_lines_ls, 'roberta15lg': sentiment_all_ls} \n",
        "  sentiment_df = pd.DataFrame(sentiment_dt) \n",
        "  sentiment_df.to_csv(filename_sentiment)\n",
        "\n",
        "  print(f'Wrote Sentiment File: {filename_sentiment}')\n",
        "  # with open(filename_clean, 'w+') as fp:\n",
        "  #   fp.write(file_clean_str)\n",
        "  \n",
        "  # print(f'File #{i}: {filename_text}\\n  Cleaned: {filename_clean}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTLV5VsuEuyJ"
      },
      "source": [
        "# **Plot Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RWQ9WDGyT2D"
      },
      "source": [
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN-qN5C-47hY"
      },
      "source": [
        "#@title Enter the Sliding Window width as Percent of Novel length (default 10%, larger=smoother)\n",
        "\n",
        "window_percent = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "win_xper = int(window_percent/100 * novel_df.shape[0])\n",
        "\n",
        "vader_col = f'vader_sma{window_percent}'\n",
        "novel_df[vader_col] = novel_df['vader'].rolling(win_xper, center=True, min_periods=1).mean()\n",
        "\n",
        "textblob_col = f'textblob_sma{window_percent}'\n",
        "novel_df[textblob_col] = novel_df['textblob'].rolling(win_xper, center=True, min_periods=1).mean()\n",
        "novel_df.plot(y=[vader_col, textblob_col])\n",
        "\n",
        "plt.title(f'{Novel_Title}\\n Sentiment Analysis (SMA {window_percent}%)')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.xlabel('Sentence No.')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(loc='best')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZqkyFA1MLzU"
      },
      "source": [
        "novel_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsyyo-oIF2V5"
      },
      "source": [
        "# **Crux Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNsXDkfi0Mk0"
      },
      "source": [
        "## **Scipy Signal Find_Peaks**\n",
        "\n",
        "* https://stackoverflow.com/questions/1713335/peak-finding-algorithm-for-python-scipy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6rvEfvz0yN"
      },
      "source": [
        "from scipy.signal import find_peaks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qWYkdaw82U2"
      },
      "source": [
        "#@title Which Lexicon?\n",
        "\n",
        "Sentiment_Model = \"VADER\" #@param [\"VADER\", \"TextBlob\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkX5inEzkHiS",
        "cellView": "form"
      },
      "source": [
        "#@title Tune the main Hyperparameter for each of the 4 Peak Detection Algorithms:\n",
        "\n",
        "Distance_Min = 380 #@param {type:\"slider\", min:50, max:1000, step:10}\n",
        "Prominence_Min = 0.013 #@param {type:\"slider\", min:0.001, max:0.05, step:0.001}\n",
        "Width_Min = 265 #@param {type:\"slider\", min:10, max:500, step:5}\n",
        "Threshold_Min = 0.0005 #@param {type:\"slider\", min:0.0001, max:0.002, step:0.0001}\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [30, 20]\n",
        "\n",
        "model_name = f'{Sentiment_Model.lower()}_sma{window_percent}'\n",
        "\n",
        "x = novel_df[model_name]\n",
        "\n",
        "# Peak Algo #1 (by Distance)\n",
        "distance_min = Distance_Min # 750\n",
        "\n",
        "# Peak Algo #2 (by Prominence)\n",
        "prominence_min = Prominence_Min # 0.01\n",
        "\n",
        "# Peak Algo #3 (by Width)\n",
        "width_min = Width_Min # 175\n",
        "\n",
        "# Peak Algo #4 (by Threshold)\n",
        "threshold_min = Threshold_Min # 0.001\n",
        "\n",
        "\n",
        "peaks, _ = find_peaks(x, distance=distance_min)\n",
        "peaks2, _ = find_peaks(x, prominence=prominence_min)      # BEST!\n",
        "peaks3, _ = find_peaks(x, width=width_min)\n",
        "peaks4, _ = find_peaks(x, threshold=threshold_min)     # Required vertical distance to its direct neighbouring samples, pretty useless\n",
        "\n",
        "\n",
        "x_inv = pd.Series([-x for x in novel_df[model_name].to_list()])\n",
        "\n",
        "valleys, _ = find_peaks(x_inv, distance=distance_min)\n",
        "valleys2, _ = find_peaks(x_inv, prominence=prominence_min)      # BEST!\n",
        "valleys3, _ = find_peaks(x_inv, width=width_min)\n",
        "valleys4, _ = find_peaks(x_inv, threshold=threshold_min)     # Required vertical distance to its direct neighbouring samples, pretty useless\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.plot(x)\n",
        "plt.title(f'Distance Peak Detection\\n Distance Minimum={distance_min}')\n",
        "plt.plot(peaks, x[peaks], \"^g\", markersize=7)\n",
        "plt.plot(valleys, x[valleys], \"vr\", markersize=7)\n",
        "for x_val in peaks:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='bottom', rotation=90, size='large', color='black', weight='semibold')\n",
        "for x_val in valleys:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='top', rotation=270, size='large', color='black', weight='semibold')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.plot(x)\n",
        "plt.title(f'Prominence Peak Detection\\n Prominence Minimum={prominence_min}')\n",
        "plt.plot(peaks2, x[peaks2], \"^g\", markersize=7)\n",
        "plt.plot(valleys2, x[valleys2], \"vr\", markersize=7)\n",
        "for x_val in peaks2:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='bottom', rotation=90, size='large', color='black', weight='semibold')\n",
        "for x_val in valleys2:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='top', rotation=270, size='large', color='black', weight='semibold')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.plot(x)\n",
        "plt.title(f'Width Peak Detection\\n Width Minimum={width_min}')\n",
        "plt.plot(valleys3, x[valleys3], \"vr\", markersize=7)\n",
        "plt.plot(peaks3, x[peaks3], \"^g\", markersize=7)\n",
        "for x_val in peaks3:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='bottom', rotation=90, size='large', color='black', weight='semibold')\n",
        "for x_val in valleys3:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='top', rotation=270, size='large', color='black', weight='semibold')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.plot(x)\n",
        "plt.title(f'Threshold Peak Detection\\n Threshold Minimum={threshold_min}')\n",
        "plt.plot(valleys4, x[valleys4], \"vr\", markersize=7)\n",
        "plt.plot(valleys4, x[valleys4], \"^g\", markersize=7)\n",
        "for x_val in peaks4:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='bottom', rotation=90, size='large', color='black', weight='semibold')\n",
        "for x_val in valleys4:\n",
        "  plt.text(x_val, x[x_val], f'-----{x_val}', ha='center', va='top', rotation=270, size='large', color='black', weight='semibold')\n",
        "\n",
        "plt.suptitle(f'{Novel_Title}\\n Peak Detection of Sentiment Analysis (SMA {window_percent}%)', fontsize=20)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-jc8YkYz154"
      },
      "source": [
        "#@title Select a Peak Detection Algorithms to View in Detail (usually Distance or Width is best):\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "\n",
        "Peak_Algorithm = \"Distance\" #@param [\"Distance\", \"Prominence\", \"Width\", \"Threshold\"]\n",
        "\n",
        "if Peak_Algorithm == 'Distance':\n",
        "  hyperparam = distance_min\n",
        "  peaks = peaks\n",
        "  valleys = valleys\n",
        "elif Peak_Algorithm == 'Prominence':\n",
        "  hyperparam = prominence_min\n",
        "  peaks = peaks2\n",
        "  valleys = valleys2  \n",
        "elif Peak_Algorithm == 'Width':\n",
        "  hyperparam = width_min\n",
        "  peaks = peaks3\n",
        "  valleys = valleys3\n",
        "else:\n",
        "  # Assume Peak_Algorithm == 'Threshold'\n",
        "  hyperparam = threshold_min\n",
        "  peaks = peaks4\n",
        "  valleys = valleys4\n",
        "\n",
        "# model_name = f'{Sentiment_Model.lower()}_sma10'\n",
        "\n",
        "# x = novel_clean_df[model_name]\n",
        "\n",
        "# peaks2, _ = find_peaks(x, prominence=peak_prominence)  \n",
        "\n",
        "# x_inv = pd.Series([-x for x in novel_clean_df[model_name].to_list()])\n",
        "# valleys2, _ = find_peaks(x_inv, prominence=peak_prominence)     \n",
        "\n",
        "plt.plot(x)\n",
        "plt.plot(peaks, x[peaks], \"^g\", markersize=15, label='peak sentence#')\n",
        "plt.plot(valleys, x[valleys], \"vr\", markersize=15, label='valley sentence#')\n",
        "for x_val in peaks:\n",
        "  plt.text(x_val, x[x_val], f'    {x_val}', horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
        "for x_val in valleys:\n",
        "  plt.text(x_val, x[x_val], f'    {x_val}', horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
        "plt.title(f'{Novel_Title}\\n Sentiment Analysis (SMA {window_percent}%) \\n Peak Detection using {Peak_Algorithm} Algorithm with Hyperparameter={hyperparam}', fontsize=16)\n",
        "plt.ylabel('Sentiment')\n",
        "plt.xlabel('Sentence No.')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "filename_plot = f\"cruxes_plot_{Novel_Title.replace(' ', '_')}.png\"\n",
        "plt.savefig(filename_plot, dpi=300)\n",
        "plt.show();\n",
        "\n",
        "print(f'\\n\\n     >>>>> SAVED PLOT TO FILE: [{filename_plot}] <<<<<')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Nhk1TncLq5"
      },
      "source": [
        "# Download Crux Point Plot file 'crux_plot.png' to your laptop\n",
        "\n",
        "files.download(filename_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6bmv1yCw4UB"
      },
      "source": [
        "# **Get Context around Crux Points**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgIQYvcP4ko8"
      },
      "source": [
        "#@title How many Sentences around Crux Point do you want to view for context?\n",
        "\n",
        "Crux_Sentence_Context_Count = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ov-ywq0XHQL"
      },
      "source": [
        "**[NOTE] May have to run 2-3x times to save file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtZGmbp0x5Ny"
      },
      "source": [
        "peaks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad86Yzl-E_yu"
      },
      "source": [
        "%%capture cap --no-stderr\n",
        "\n",
        "# Print Context around each Sentiment Peak\n",
        "\n",
        "novel_sent_len = novel_df.shape[0]\n",
        "halfwin = int(Crux_Sentence_Context_Count/2)\n",
        "crux_sents_ls = []\n",
        "nl = '\\n'\n",
        "\n",
        "print('==================================================')\n",
        "print('============     Peak Crux Points   ==============')\n",
        "print('==================================================\\n\\n')\n",
        "\n",
        "# for i, apeak in enumerate(peaks2):\n",
        "for i, apeak in enumerate(peaks):\n",
        "  crux_sents_ls = []\n",
        "  win_start = max(0, apeak-halfwin)\n",
        "  win_end = min(apeak+halfwin+1, novel_sent_len)\n",
        "  # for sent_idx in range(apeak-halfwin,apeak+halfwin+1):\n",
        "  for sent_idx in range(win_start,win_end):\n",
        "\n",
        "    sent_cur = novel_df.iloc[sent_idx].text_raw\n",
        "    if sent_idx == apeak:\n",
        "      sent_str = sent_cur.upper()\n",
        "    else:\n",
        "      sent_str = sent_cur\n",
        "    crux_sents_ls.append(sent_str)\n",
        "  \n",
        "  # context_ls = novel_df.iloc[apeak-halfwin:apeak+halfwin].text_raw\n",
        "  print(f\"Peak #{i} at Sentence #{apeak}:\\n\\n{nl.join(crux_sents_ls)}\\n\\n\\n\")\n",
        "\n",
        "print('==================================================')\n",
        "print('===========     Crux Valley Points    ============')\n",
        "print('==================================================\\n\\n')\n",
        "\n",
        "\n",
        "# for i, avalley in enumerate(valleys2):\n",
        "for i, avalley in enumerate(valleys):\n",
        "  crux_sents_ls = []\n",
        "  win_start = max(0, avalley-halfwin)\n",
        "  win_end = min(avalley+halfwin+1, novel_sent_len)\n",
        "  # for sent_idx in range(avalley-halfwin,avalley+halfwin+1):\n",
        "  for sent_idx in range(win_start,win_end):\n",
        "    sent_cur = novel_df.iloc[sent_idx].text_raw\n",
        "    if sent_idx == avalley:\n",
        "      sent_str = sent_cur.upper()\n",
        "    else:\n",
        "      sent_str = sent_cur\n",
        "    crux_sents_ls.append(sent_str)\n",
        "\n",
        "  # context_ls = novel_df.iloc[avalley-halfwin:avalley+halfwin].text_raw\n",
        "  print(f\"Valley #{i} at Sentence #{avalley}:\\n\\n{nl.join(crux_sents_ls)}\\n\\n\\n\")\n",
        "\n",
        "filename_cruxes = f\"cruxes_context_{Novel_Title.replace(' ', '_')}.txt\" \n",
        "\n",
        "with open(filename_cruxes, 'w') as f:\n",
        "    f.write(str(cap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2OSRfUF3el1"
      },
      "source": [
        "# Download Crux Point Report file 'cruxes.txt' to your laptop\n",
        "\n",
        "files.download(filename_cruxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIclXwPYXYm9"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obahB2FESm_O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}